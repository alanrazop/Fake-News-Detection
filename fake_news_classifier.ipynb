{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYtnlFifggLN",
        "outputId": "630115cb-0b31-4275-a931-d529dfcbcad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Montado en Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HKpbvougEk6",
        "outputId": "87712dee-83b6-4476-8b3f-e8bd8864019b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Aplicaciones Avanzadas/DataBank\n",
            "Fake.csv\t\t    fake_news_model.keras    scaler.pkl\n",
            "fake_news_classifier.h5     label_encoder.pkl\t     tfidf_vectorizer.pkl\n",
            "fake_news_classifier.keras  noticia_prueba_fake.txt  True.csv\n",
            "fake_news_dataset.csv\t    noticia_prueba_real.txt\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/drive/MyDrive/Aplicaciones Avanzadas/DataBank\"\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FHOFGMC0gEuw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE-92DIPhqF6",
        "outputId": "88dbfbf3-2b0f-45ac-994b-4d3f612c5ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Descargar stopwords (palabras comunes) (solo la primera vez)\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Lemming (Lenguage informal)\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo, vectorizer, scaler, y label encoder\n",
        "\n",
        "loaded_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "loaded_scaler = joblib.load('scaler.pkl')\n",
        "loaded_le = joblib.load('label_encoder.pkl')\n",
        "loaded_model = tf.keras.models.load_model('fake_news_classifier.keras')"
      ],
      "metadata": {
        "id": "xHfuCB8XC8vT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir funciones de preprocesamiento de texto\n",
        "\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r'\\d+', '', texto)\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)\n",
        "    texto = texto.strip()\n",
        "    texto = re.sub(r'\\s+', ' ', texto)\n",
        "    palabras = texto.split()\n",
        "    palabras = [p for p in palabras if p not in stop_words]\n",
        "    return ' '.join(palabras)\n",
        "\n",
        "try:\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatizar_texto(texto):\n",
        "  palabras = texto.split()\n",
        "  palabras = [lemmatizer.lemmatize(p) for p in palabras]\n",
        "  return ' '.join(palabras)\n",
        "\n",
        "# Función para preprocesar un texto nuevo\n",
        "def preprocess_text_for_prediction(text):\n",
        "    cleaned_text = limpiar_texto(text)\n",
        "    lemmatized_text = lemmatizar_texto(cleaned_text)\n",
        "    text_tfidf = loaded_vectorizer.transform([lemmatized_text])\n",
        "    text_scaled = loaded_scaler.transform(text_tfidf.toarray())\n",
        "    return text_scaled"
      ],
      "metadata": {
        "id": "BlkoJSFTDCtY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XNqMgHilWVp7"
      },
      "outputs": [],
      "source": [
        "# Function para predecir\n",
        "def predict_from_text_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        processed_text = preprocess_text_for_prediction(text)\n",
        "\n",
        "        prediction_prob = loaded_model.predict(processed_text).flatten()[0]\n",
        "        prediction = (prediction_prob >= 0.5).astype(int)\n",
        "\n",
        "        predicted_label = loaded_le.inverse_transform([prediction])[0]\n",
        "\n",
        "        print(f\"Archivo: {file_path}\")\n",
        "        print(f\"Probabilidad de ser Real: {prediction_prob:.4f}\")\n",
        "        print(f\"Predicción: {'Verdadero' if predicted_label == 'real' else 'Falso'}\")\n",
        "        return predicted_label\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Archivo no encontrado en {file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar el archivo {file_path}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-'*42)\n",
        "print('Archivos reales')\n",
        "print('-'*42)\n",
        "# Noticia real de sky news\n",
        "predict_from_text_file('/content/real_example.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "LMSBJOODC1ZI",
        "outputId": "dea3396c-dd5f-4fba-8c77-6694912cb957"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------\n",
            "Archivos reales\n",
            "------------------------------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Archivo: /content/real_example.txt\n",
            "Probabilidad de ser Real: 0.8693\n",
            "Predicción: Verdadero\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'real'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_from_text_file('/content/real_example2.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "U8w9-U4LEnzD",
        "outputId": "cf7751e8-a123-41d0-862f-8bf4ba0f2e4c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Archivo: /content/real_example2.txt\n",
            "Probabilidad de ser Real: 0.5092\n",
            "Predicción: Verdadero\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'real'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_from_text_file('/content/article_real.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "yp_4OGYuzHK8",
        "outputId": "dadccad5-63c1-4241-ec68-633e99f31882"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "Archivo: /content/article_real.txt\n",
            "Probabilidad de ser Real: 0.0000\n",
            "Predicción: Falso\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fake'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}